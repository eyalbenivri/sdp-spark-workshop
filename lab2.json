{"paragraphs":[{"text":"%pyspark\ndatadir = \"/home/spark/Documents/spark-workshop/data/\"","dateUpdated":"Jun 22, 2016 7:49:15 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466614121785_1605303706","id":"20160622-194841_1053029930","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Jun 22, 2016 7:48:41 PM","dateStarted":"Jun 22, 2016 7:49:15 PM","dateFinished":"Jun 22, 2016 7:49:15 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:259"},{"text":"%md\nIn this lab, you will analyze a real-world dataset -- information about US flight delays in January 2016, courtesy of the United States Department of Transportation. You can [download additional datasets](http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time) later. Here's another example you might find interesting -- [US border crossing/entry data per port of entry](http://transborder.bts.gov/programs/international/transborder/TBDR_BC/TBDR_BCQ.html).\n","dateUpdated":"Jun 22, 2016 7:39:43 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613202387_1676808700","id":"20160622-193322_1138472819","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In this lab, you will analyze a real-world dataset &ndash; information about US flight delays in January 2016, courtesy of the United States Department of Transportation. You can <a href=\"http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;DB_Short_Name=On-Time\">download additional datasets</a> later. Here's another example you might find interesting &ndash; <a href=\"http://transborder.bts.gov/programs/international/transborder/TBDR_BC/TBDR_BCQ.html\">US border crossing/entry data per port of entry</a>.</p>\n"},"dateCreated":"Jun 22, 2016 7:33:22 PM","dateStarted":"Jun 22, 2016 7:39:39 PM","dateFinished":"Jun 22, 2016 7:39:39 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:260"},{"text":"%md\n\n#### Task 1: Inspecting the Data\n\nThis dataset ships with two files (in the `~/data` directory, if you are using the instructor-provided VirtualBox appliance). First, the `airline-format.html` file contains a brief description of the dataset, and the various data fields. For example, the `ArrDelay` field is the flight's arrival delay, in minutes. Second, the `airline-delays.csv` file is a comma-separated collection of flight records, one record per line.\n\nInspect the fields described in the `airline-format.html` file. Make a note of fields that describe the flight, its origin and destination airports, and any delays encountered on departure and arrival.\n\nLet's start by counting the number of records in our dataset. Run the following command in a terminal window:\n\n```\nwc -l airline-delays.csv\n```\n\nThis dataset has hundreds of thousands of records. To sample 10 records from the dataset picked at probability 0.005%, run the following command (for convenience, its output is also quoted here):\n\n```\ncat airline-delays.csv | cut -d',' -f1-20 | awk '{ if (rand() <= 0.00005 || FNR==1) { print $0; if (++count > 11) exit; } }'\n\"Year\",\"Quarter\",\"Month\",\"DayofMonth\",\"DayOfWeek\",\"FlightDate\",\"UniqueCarrier\",\"AirlineID\",\"Carrier\",\"TailNum\",\"FlightNum\",\"OriginAirportID\",\"OriginAirportSeqID\",\"OriginCityMarketID\",\"Origin\",\"OriginCityName\",\"OriginState\",\"OriginStateFips\",\"OriginStateName\",\"OriginWac\"\n2016,1,1,20,3,2016-01-20,\"AA\",19805,\"AA\",\"N3AFAA\",\"242\",14771,1477102,32457,\"SFO\",\"San Francisco, CA\",\"CA\",\"06\",\"California\"\n2016,1,1,9,6,2016-01-09,\"AA\",19805,\"AA\",\"N859AA\",\"284\",12173,1217302,32134,\"HNL\",\"Honolulu, HI\",\"HI\",\"15\",\"Hawaii\"\n2016,1,1,9,6,2016-01-09,\"AA\",19805,\"AA\",\"N3GRAA\",\"1227\",11278,1127803,30852,\"DCA\",\"Washington, DC\",\"VA\",\"51\",\"Virginia\"\n2016,1,1,4,1,2016-01-04,\"AA\",19805,\"AA\",\"N3BGAA\",\"1450\",11298,1129804,30194,\"DFW\",\"Dallas/Fort Worth, TX\",\"TX\",\"48\",\"Texas\"\n2016,1,1,5,2,2016-01-05,\"AA\",19805,\"AA\",\"N3AMAA\",\"1616\",11298,1129804,30194,\"DFW\",\"Dallas/Fort Worth, TX\",\"TX\",\"48\",\"Texas\"\n2016,1,1,20,3,2016-01-20,\"AA\",19805,\"AA\",\"N916US\",\"1783\",11057,1105703,31057,\"CLT\",\"Charlotte, NC\",\"NC\",\"37\",\"North Carolina\"\n2016,1,1,2,6,2016-01-02,\"AS\",19930,\"AS\",\"N517AS\",\"879\",14747,1474703,30559,\"SEA\",\"Seattle, WA\",\"WA\",\"53\",\"Washington\"\n2016,1,1,20,3,2016-01-20,\"AS\",19930,\"AS\",\"N769AS\",\"568\",14057,1405702,34057,\"PDX\",\"Portland, OR\",\"OR\",\"41\",\"Oregon\"\n2016,1,1,24,7,2016-01-24,\"UA\",19977,\"UA\",\"\",\"706\",14843,1484304,34819,\"SJU\",\"San Juan, PR\",\"PR\",\"72\",\"Puerto Rico\"\n2016,1,1,15,5,2016-01-15,\"UA\",19977,\"UA\",\"N34460\",\"1077\",12266,1226603,31453,\"IAH\",\"Houston, TX\",\"TX\",\"48\",\"Texas\"\n2016,1,1,12,2,2016-01-12,\"UA\",19977,\"UA\",\"N423UA\",\"1253\",13303,1330303,32467,\"MIA\",\"Miami, FL\",\"FL\",\"12\",\"Florida\"\n```\n\nThis displays the first 20 fields of the 10 sampled records from the file. The first line is a header line, so we printed it unconditionally. This is a typical example of structured data that we would have to parse first before analyzing it with Spark.\n\n> We could examine the full dataset using shell commands, because it is not exceptionally big. For larger datasets that couldn't conceivably be processed or even stored on a single machine, we could have used Spark itself to perform the sampling. If you're interested, examine the `takeSample` method that Spark RDDs provide.\n\n","dateUpdated":"Jun 22, 2016 7:37:44 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613236558_-238978667","id":"20160622-193356_1234679826","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Task 1: Inspecting the Data</h4>\n<p>This dataset ships with two files (in the <code>~/data</code> directory, if you are using the instructor-provided VirtualBox appliance). First, the <code>airline-format.html</code> file contains a brief description of the dataset, and the various data fields. For example, the <code>ArrDelay</code> field is the flight's arrival delay, in minutes. Second, the <code>airline-delays.csv</code> file is a comma-separated collection of flight records, one record per line.</p>\n<p>Inspect the fields described in the <code>airline-format.html</code> file. Make a note of fields that describe the flight, its origin and destination airports, and any delays encountered on departure and arrival.</p>\n<p>Let's start by counting the number of records in our dataset. Run the following command in a terminal window:</p>\n<pre><code>wc -l airline-delays.csv\n</code></pre>\n<p>This dataset has hundreds of thousands of records. To sample 10 records from the dataset picked at probability 0.005%, run the following command (for convenience, its output is also quoted here):</p>\n<pre><code>cat airline-delays.csv | cut -d',' -f1-20 | awk '{ if (rand() &lt;= 0.00005 || FNR==1) { print $0; if (++count &gt; 11) exit; } }'\n\"Year\",\"Quarter\",\"Month\",\"DayofMonth\",\"DayOfWeek\",\"FlightDate\",\"UniqueCarrier\",\"AirlineID\",\"Carrier\",\"TailNum\",\"FlightNum\",\"OriginAirportID\",\"OriginAirportSeqID\",\"OriginCityMarketID\",\"Origin\",\"OriginCityName\",\"OriginState\",\"OriginStateFips\",\"OriginStateName\",\"OriginWac\"\n2016,1,1,20,3,2016-01-20,\"AA\",19805,\"AA\",\"N3AFAA\",\"242\",14771,1477102,32457,\"SFO\",\"San Francisco, CA\",\"CA\",\"06\",\"California\"\n2016,1,1,9,6,2016-01-09,\"AA\",19805,\"AA\",\"N859AA\",\"284\",12173,1217302,32134,\"HNL\",\"Honolulu, HI\",\"HI\",\"15\",\"Hawaii\"\n2016,1,1,9,6,2016-01-09,\"AA\",19805,\"AA\",\"N3GRAA\",\"1227\",11278,1127803,30852,\"DCA\",\"Washington, DC\",\"VA\",\"51\",\"Virginia\"\n2016,1,1,4,1,2016-01-04,\"AA\",19805,\"AA\",\"N3BGAA\",\"1450\",11298,1129804,30194,\"DFW\",\"Dallas/Fort Worth, TX\",\"TX\",\"48\",\"Texas\"\n2016,1,1,5,2,2016-01-05,\"AA\",19805,\"AA\",\"N3AMAA\",\"1616\",11298,1129804,30194,\"DFW\",\"Dallas/Fort Worth, TX\",\"TX\",\"48\",\"Texas\"\n2016,1,1,20,3,2016-01-20,\"AA\",19805,\"AA\",\"N916US\",\"1783\",11057,1105703,31057,\"CLT\",\"Charlotte, NC\",\"NC\",\"37\",\"North Carolina\"\n2016,1,1,2,6,2016-01-02,\"AS\",19930,\"AS\",\"N517AS\",\"879\",14747,1474703,30559,\"SEA\",\"Seattle, WA\",\"WA\",\"53\",\"Washington\"\n2016,1,1,20,3,2016-01-20,\"AS\",19930,\"AS\",\"N769AS\",\"568\",14057,1405702,34057,\"PDX\",\"Portland, OR\",\"OR\",\"41\",\"Oregon\"\n2016,1,1,24,7,2016-01-24,\"UA\",19977,\"UA\",\"\",\"706\",14843,1484304,34819,\"SJU\",\"San Juan, PR\",\"PR\",\"72\",\"Puerto Rico\"\n2016,1,1,15,5,2016-01-15,\"UA\",19977,\"UA\",\"N34460\",\"1077\",12266,1226603,31453,\"IAH\",\"Houston, TX\",\"TX\",\"48\",\"Texas\"\n2016,1,1,12,2,2016-01-12,\"UA\",19977,\"UA\",\"N423UA\",\"1253\",13303,1330303,32467,\"MIA\",\"Miami, FL\",\"FL\",\"12\",\"Florida\"\n</code></pre>\n<p>This displays the first 20 fields of the 10 sampled records from the file. The first line is a header line, so we printed it unconditionally. This is a typical example of structured data that we would have to parse first before analyzing it with Spark.</p>\n<blockquote><p>We could examine the full dataset using shell commands, because it is not exceptionally big. For larger datasets that couldn't conceivably be processed or even stored on a single machine, we could have used Spark itself to perform the sampling. If you're interested, examine the <code>takeSample</code> method that Spark RDDs provide.</p>\n</blockquote>\n"},"dateCreated":"Jun 22, 2016 7:33:56 PM","dateStarted":"Jun 22, 2016 7:37:40 PM","dateFinished":"Jun 22, 2016 7:37:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:261"},{"text":"%md\n\n#### Task 2: Parsing CSV Data\n\nNext, you have to parse the CSV data. The header line provides the column names, and then each subsequent line can be parsed taking these into account. Python has a built-in `csv` module (unrelated to Spark) that you can use to parse CSV lines. Try it out in a Python shell (by running `python` in a terminal window) or the Pyspark shell (by running `bin/pyspark` from Spark's installation directory in a terminal window):\n\n```python\nimport csv\nfrom StringIO import StringIO\n\nsi = StringIO('\"Alice\",14,\"panda\"')\nfields = [\"name\", \"age\", \"favorite animal\"]\ncsv.DictReader(si, fieldnames=fields).next()\n```\n\nGreat! Next, write a function that parses one line from the flight delays CSV file. You can call that function `parseLine`, and it should return the Python dict that `DictReader.next` returns.\n\n**Solution**:\n\n```python\ndef parseLine(line, fieldnames):\n    si = StringIO(line)\n    return csv.DictReader(si, fieldnames=fieldnames).next()\n```\n\nNext, create an RDD based on the `airline-delays.csv` file, and map each line of that file using the `parseLine` function you wrote. The result should be an RDD of Python dicts representing the flight delay data. Note that the first line (the header line) should be discarded.\n\n**Solution**:\n\n```python\nrdd = sc.textFile(\"/home/spark/Documents/spark-workshop/data/airline-delays.csv\")\nheaderline = rdd.first()\nfieldnames = filter(lambda field: len(field) > 0,\n                map(lambda field: field.strip('\"'), headerline.split(',')))\nflights = rdd.filter(lambda line: line != headerline)       \\\n             .map(lambda line: parseLine(line, fieldnames))\nflights.persist()\n```\n","dateUpdated":"Jun 22, 2016 7:40:42 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613271503_875300120","id":"20160622-193431_1294075057","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Task 2: Parsing CSV Data</h4>\n<p>Next, you have to parse the CSV data. The header line provides the column names, and then each subsequent line can be parsed taking these into account. Python has a built-in <code>csv</code> module (unrelated to Spark) that you can use to parse CSV lines. Try it out in a Python shell (by running <code>python</code> in a terminal window) or the Pyspark shell (by running <code>bin/pyspark</code> from Spark's installation directory in a terminal window):</p>\n<pre><code class=\"python\">import csv\nfrom StringIO import StringIO\n\nsi = StringIO('\"Alice\",14,\"panda\"')\nfields = [\"name\", \"age\", \"favorite animal\"]\ncsv.DictReader(si, fieldnames=fields).next()\n</code></pre>\n<p>Great! Next, write a function that parses one line from the flight delays CSV file. You can call that function <code>parseLine</code>, and it should return the Python dict that <code>DictReader.next</code> returns.</p>\n<p><strong>Solution</strong>:</p>\n<pre><code class=\"python\">def parseLine(line, fieldnames):\n    si = StringIO(line)\n    return csv.DictReader(si, fieldnames=fieldnames).next()\n</code></pre>\n<p>Next, create an RDD based on the <code>airline-delays.csv</code> file, and map each line of that file using the <code>parseLine</code> function you wrote. The result should be an RDD of Python dicts representing the flight delay data. Note that the first line (the header line) should be discarded.</p>\n<p><strong>Solution</strong>:</p>\n<pre><code class=\"python\">rdd = sc.textFile(\"/home/spark/Documents/spark-workshop/data/airline-delays.csv\")\nheaderline = rdd.first()\nfieldnames = filter(lambda field: len(field) &gt; 0,\n                map(lambda field: field.strip('\"'), headerline.split(',')))\nflights = rdd.filter(lambda line: line != headerline)       \\\n             .map(lambda line: parseLine(line, fieldnames))\nflights.persist()\n</code></pre>\n"},"dateCreated":"Jun 22, 2016 7:34:31 PM","dateStarted":"Jun 22, 2016 7:40:40 PM","dateFinished":"Jun 22, 2016 7:40:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:262"},{"text":"%md\n\n#### Task 3: Querying Flights and Delays\n\nNow that you have the flight objects, it's time to perform a few queries and gather some useful information. Suppose you're in Boston, MA. Which airline has the most flights departing from Boston?\n","dateUpdated":"Jun 22, 2016 7:37:44 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613324061_-212122865","id":"20160622-193524_678198312","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Task 3: Querying Flights and Delays</h4>\n<p>Now that you have the flight objects, it's time to perform a few queries and gather some useful information. Suppose you're in Boston, MA. Which airline has the most flights departing from Boston?</p>\n"},"dateCreated":"Jun 22, 2016 7:35:24 PM","dateStarted":"Jun 22, 2016 7:37:40 PM","dateFinished":"Jun 22, 2016 7:37:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:263"},{"text":"%md\n\nOverall, which airline has the worst average delay? How bad was that delay?\n\n> **HINT**: Use `combineByKey`.\n","dateUpdated":"Jun 22, 2016 7:37:44 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613358240_-588361317","id":"20160622-193558_1621941484","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Overall, which airline has the worst average delay? How bad was that delay?</p>\n<blockquote><p><strong>HINT</strong>: Use <code>combineByKey</code>.</p>\n</blockquote>\n"},"dateCreated":"Jun 22, 2016 7:35:58 PM","dateStarted":"Jun 22, 2016 7:37:40 PM","dateFinished":"Jun 22, 2016 7:37:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:264"},{"text":"%md\nLiving in Chicago, IL, what are the farthest 10 destinations that you could fly to? (Note that our dataset contains only US domestic flights.)\n\n","dateUpdated":"Jun 22, 2016 7:37:44 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613381083_1177064206","id":"20160622-193621_1343215794","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Living in Chicago, IL, what are the farthest 10 destinations that you could fly to? (Note that our dataset contains only US domestic flights.)</p>\n"},"dateCreated":"Jun 22, 2016 7:36:21 PM","dateStarted":"Jun 22, 2016 7:37:41 PM","dateFinished":"Jun 22, 2016 7:37:41 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:265"},{"text":"%md\n\nSuppose you're in New York, NY and are contemplating direct flights to San Francisco, CA. In terms of arrival delay, which airline has the best record on that route?\n","dateUpdated":"Jun 22, 2016 7:37:44 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613403334_-579283874","id":"20160622-193643_613393677","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Suppose you're in New York, NY and are contemplating direct flights to San Francisco, CA. In terms of arrival delay, which airline has the best record on that route?</p>\n"},"dateCreated":"Jun 22, 2016 7:36:43 PM","dateStarted":"Jun 22, 2016 7:37:41 PM","dateFinished":"Jun 22, 2016 7:37:41 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:266"},{"text":"%md\n### Discussion\n\nSuppose you had to calculate multiple aggregated values from the `flights` RDD -- e.g., the average arrival delay, the average departure delay, and the average flight duration for flights from Boston. How would you express it using SQL, if `flights` was a table in a relational database? How would you express it using transformations and actions on RDDs? Which is easier to develop and maintain?\n","dateUpdated":"Jun 22, 2016 7:37:44 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613450222_366706402","id":"20160622-193730_2025689839","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Discussion</h3>\n<p>Suppose you had to calculate multiple aggregated values from the <code>flights</code> RDD &ndash; e.g., the average arrival delay, the average departure delay, and the average flight duration for flights from Boston. How would you express it using SQL, if <code>flights</code> was a table in a relational database? How would you express it using transformations and actions on RDDs? Which is easier to develop and maintain?</p>\n"},"dateCreated":"Jun 22, 2016 7:37:30 PM","dateStarted":"Jun 22, 2016 7:37:41 PM","dateFinished":"Jun 22, 2016 7:37:41 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:267"},{"dateUpdated":"Jun 22, 2016 7:37:44 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613461122_-872185057","id":"20160622-193741_1426930913","dateCreated":"Jun 22, 2016 7:37:41 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:268"}],"name":"Lab 2 - Flight Delay Analysis","id":"2BNS6VQBY","angularObjects":{"2BPTZXB14":[],"2BMJRDTW3":[],"2BKPUUQTH":[],"2BKSECM3A":[],"2BN9ZJDBP":[],"2BKNGQUUG":[],"2BKAYKVR2":[],"2BKSAS565":[],"2BMPZT5XH":[],"2BNWNU8NC":[],"2BQ1AJWE4":[],"2BN44SKDU":[],"2BN59C6S8":[],"2BP824Q3U":[]},"config":{"looknfeel":"default"},"info":{}}