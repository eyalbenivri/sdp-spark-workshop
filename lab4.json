{"paragraphs":[{"text":"%pyspark\ndatadir = \"/home/spark/Documents/spark-workshop/data/\"","dateUpdated":"Jun 22, 2016 8:27:57 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466614711030_1464679890","id":"20160622-195831_1982475144","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Jun 22, 2016 7:58:31 PM","dateStarted":"Jun 22, 2016 7:58:48 PM","dateFinished":"Jun 22, 2016 7:58:48 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:703"},{"text":"%md\nIn this lab, you will work with another real-world dataset that contains residential property sales across the UK, as reported to the Land Registry. You can download this dataset and many others from [data.gov.uk](https://data.gov.uk/dataset/land-registry-monthly-price-paid-data).","dateUpdated":"Jun 22, 2016 8:05:05 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466614728247_-963070346","id":"20160622-195848_1479638650","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In this lab, you will work with another real-world dataset that contains residential property sales across the UK, as reported to the Land Registry. You can download this dataset and many others from <a href=\"https://data.gov.uk/dataset/land-registry-monthly-price-paid-data\">data.gov.uk</a>.</p>\n"},"dateCreated":"Jun 22, 2016 7:58:48 PM","dateStarted":"Jun 22, 2016 7:59:06 PM","dateFinished":"Jun 22, 2016 7:59:06 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:704"},{"text":"%md\n\n#### Task 1: Inspecting the Data\n\nAs always, we begin by inspecting the data, which is in the `~/data/prop-prices.csv` file. Run the following command to take a look at some of the entries:\n\n```\nhead ~/data/prop-prices.csv\n```\n\nNote that this time, the CSV file does not have headers. To determine which fields are available, consult the [guidance page](https://www.gov.uk/guidance/about-the-price-paid-data).","dateUpdated":"Jun 22, 2016 8:05:05 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466614746672_1002458543","id":"20160622-195906_1906347412","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Task 1: Inspecting the Data</h4>\n<p>As always, we begin by inspecting the data, which is in the <code>~/data/prop-prices.csv</code> file. Run the following command to take a look at some of the entries:</p>\n<pre><code>head ~/data/prop-prices.csv\n</code></pre>\n<p>Note that this time, the CSV file does not have headers. To determine which fields are available, consult the <a href=\"https://www.gov.uk/guidance/about-the-price-paid-data\">guidance page</a>.</p>\n"},"dateCreated":"Jun 22, 2016 7:59:06 PM","dateStarted":"Jun 22, 2016 7:59:23 PM","dateFinished":"Jun 22, 2016 7:59:23 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:705"},{"text":"%md\n#### Task 2: Importing the Data\n\nIn a previous lab, we used the Python `csv` module to parse CSV files. However, because we're working with structured data, the Spark SQL framework can be easier to use and provide better performance. We are going to use the `pyspark_csv` third-party open source module to create a `DataFrame` from an RDD of CSV lines.\n\n> **NOTE**: The `pyspark_csv.py` file is in the `Downloads` directory on the VirtualBox appliance. You can also [download it yourself](https://github.com/seahboonsiew/pyspark-csv) and place it in some directory.\n> \n> This module also depends on the `dateutils` module, which typically doesn't ship with Python. It is already installed in the VirtualBox appliance. To install it on your own machine, run the following from a terminal window:\n\n```bash\nsudo dpkg --configure -a\nsudo apt-get install python-setuptools\nsudo easy_install dateutils\n```\n\n\nTo import `pyspark_csv`, you'll need the following snippet of code that adds its path to the module search path, and adds it to the Spark executors so they can find it as well:\n\n```python\nimport sys\nsys.path.append('/home/spark/Downloads')   # replace as necessary\nimport pyspark_csv\nsc.addFile('/home/spark/Downloads/pyspark_csv.py')    # ditto\n```\n\nNext, load the `prop-prices.csv` file as an RDD, and use the `csvToDataFrame` function from the `pyspark_csv` module to create a `DataFrame` and register it as a temporary table so that you can run SQL queries:\n\n```python\ncolumns = ['id', 'price', 'date', 'zip', 'type', 'new', 'duration', 'PAON',\n           'SAON', 'street', 'locality', 'town', 'district', 'county', 'ppd',\n           'status']\n\nrdd = sc.textFile(\"file://\" + datadir + \"prop-prices.csv\")\ndf = pyspark_csv.csvToDataFrame(sqlContext, rdd, columns=columns)\ndf.registerTempTable(\"properties\")\ndf.persist()\n```","dateUpdated":"Jun 22, 2016 8:05:05 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466614763800_480369610","id":"20160622-195923_2039600515","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Task 2: Importing the Data</h4>\n<p>In a previous lab, we used the Python <code>csv</code> module to parse CSV files. However, because we're working with structured data, the Spark SQL framework can be easier to use and provide better performance. We are going to use the <code>pyspark_csv</code> third-party open source module to create a <code>DataFrame</code> from an RDD of CSV lines.</p>\n<blockquote><p><strong>NOTE</strong>: The <code>pyspark_csv.py</code> file is in the <code>Downloads</code> directory on the VirtualBox appliance. You can also <a href=\"https://github.com/seahboonsiew/pyspark-csv\">download it yourself</a> and place it in some directory.</p>\n<p>This module also depends on the <code>dateutils</code> module, which typically doesn't ship with Python. It is already installed in the VirtualBox appliance. To install it on your own machine, run the following from a terminal window:</p>\n</blockquote>\n<pre><code class=\"bash\">sudo dpkg --configure -a\nsudo apt-get install python-setuptools\nsudo easy_install dateutils\n</code></pre>\n<p>To import <code>pyspark_csv</code>, you'll need the following snippet of code that adds its path to the module search path, and adds it to the Spark executors so they can find it as well:</p>\n<pre><code class=\"python\">import sys\nsys.path.append('/home/spark/Downloads')   # replace as necessary\nimport pyspark_csv\nsc.addFile('/home/spark/Downloads/pyspark_csv.py')    # ditto\n</code></pre>\n<p>Next, load the <code>prop-prices.csv</code> file as an RDD, and use the <code>csvToDataFrame</code> function from the <code>pyspark_csv</code> module to create a <code>DataFrame</code> and register it as a temporary table so that you can run SQL queries:</p>\n<pre><code class=\"python\">columns = ['id', 'price', 'date', 'zip', 'type', 'new', 'duration', 'PAON',\n           'SAON', 'street', 'locality', 'town', 'district', 'county', 'ppd',\n           'status']\n\nrdd = sc.textFile(\"file://\" + datadir + \"prop-prices.csv\")\ndf = pyspark_csv.csvToDataFrame(sqlContext, rdd, columns=columns)\ndf.registerTempTable(\"properties\")\ndf.persist()\n</code></pre>\n"},"dateCreated":"Jun 22, 2016 7:59:23 PM","dateStarted":"Jun 22, 2016 8:04:24 PM","dateFinished":"Jun 22, 2016 8:04:24 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:706"},{"text":"%md\n#### Task 3: Analyzing Property Price Trends\n\nFirst, let's do some basic analysis on the data. Find how many records we have per year, and print them out sorted by year.\n","dateUpdated":"Jun 22, 2016 8:05:05 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466614793959_-1499502258","id":"20160622-195953_2042214483","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Task 3: Analyzing Property Price Trends</h4>\n<p>First, let's do some basic analysis on the data. Find how many records we have per year, and print them out sorted by year.</p>\n"},"dateCreated":"Jun 22, 2016 7:59:53 PM","dateStarted":"Jun 22, 2016 8:05:00 PM","dateFinished":"Jun 22, 2016 8:05:00 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:707"},{"text":"%md\nAll right, so everyone knows that properties in London are expensive. Find the average property price by county, and print the top 10 most expensive counties.","dateUpdated":"Jun 22, 2016 8:05:22 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466615100503_-2098898884","id":"20160622-200500_1596605024","result":{"code":"SUCCESS","type":"HTML","msg":"<p>All right, so everyone knows that properties in London are expensive. Find the average property price by county, and print the top 10 most expensive counties.</p>\n"},"dateCreated":"Jun 22, 2016 8:05:00 PM","dateStarted":"Jun 22, 2016 8:05:20 PM","dateFinished":"Jun 22, 2016 8:05:20 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:708"},{"text":"%md\nBonus: use the Python `matplotlib` module to plot the property price changes month-over-month across the entire dataset.\n\n> The `matplotlib` module is installed in the instructor-provided VirtualBox appliance. For your own system, follow the [installation instructions](http://matplotlib.org/users/installing.html).","dateUpdated":"Jun 22, 2016 8:07:38 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466615120403_-22363098","id":"20160622-200520_959050807","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Bonus: use the Python <code>matplotlib</code> module to plot the property price changes month-over-month across the entire dataset.</p>\n<blockquote><p>The <code>matplotlib</code> module is installed in the instructor-provided VirtualBox appliance. For your own system, follow the <a href=\"http://matplotlib.org/users/installing.html\">installation instructions</a>.</p>\n</blockquote>\n"},"dateCreated":"Jun 22, 2016 8:05:20 PM","dateStarted":"Jun 22, 2016 8:07:33 PM","dateFinished":"Jun 22, 2016 8:07:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:709"},{"text":"%md\n#### Discussion\n\nNow that you have experience in working with Spark SQL and `DataFrames`, what are the advantages and disadvantages of using it compared to the core RDD functionality (such as `map`, `filter`, `reduceByKey`, and so on)? Consider which approach produces more maintainable code, offers more opportunities for optimization, makes it easier to solve certain problems, and so on.\n","dateUpdated":"Jun 22, 2016 8:07:56 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466615253023_-7591042","id":"20160622-200733_1273305611","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Discussion</h4>\n<p>Now that you have experience in working with Spark SQL and <code>DataFrames</code>, what are the advantages and disadvantages of using it compared to the core RDD functionality (such as <code>map</code>, <code>filter</code>, <code>reduceByKey</code>, and so on)? Consider which approach produces more maintainable code, offers more opportunities for optimization, makes it easier to solve certain problems, and so on.</p>\n"},"dateCreated":"Jun 22, 2016 8:07:33 PM","dateStarted":"Jun 22, 2016 8:07:53 PM","dateFinished":"Jun 22, 2016 8:07:53 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:710"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466615273253_1448145554","id":"20160622-200753_1913080206","dateCreated":"Jun 22, 2016 8:07:53 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:711"}],"name":"Lab 4 - Analyzing UK Property Prices","id":"2BN92ZWVW","angularObjects":{"2BPTZXB14":[],"2BMJRDTW3":[],"2BKPUUQTH":[],"2BKSECM3A":[],"2BN9ZJDBP":[],"2BKNGQUUG":[],"2BKAYKVR2":[],"2BKSAS565":[],"2BMPZT5XH":[],"2BNWNU8NC":[],"2BQ1AJWE4":[],"2BN44SKDU":[],"2BN59C6S8":[],"2BP824Q3U":[]},"config":{"looknfeel":"default"},"info":{}}