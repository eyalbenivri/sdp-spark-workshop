{"paragraphs":[{"text":"%pyspark\ndatadir = \"/home/spark/Documents/spark-workshop/data/\"\n","dateUpdated":"Jun 22, 2016 9:21:16 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466612132523_-1795294546","id":"20160622-191532_1643487021","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Jun 22, 2016 7:15:32 PM","dateStarted":"Jun 22, 2016 9:21:16 PM","dateFinished":"Jun 22, 2016 9:22:24 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:60","focus":true},{"text":"%md\nIn this lab, you will become acquainted with your Spark installation, and run your first Spark job -- a multi-file word count.\n\n> The instructor should have explained how to install Spark on your machine. One option is to use the instructor's VirtualBox appliance, which you can import in the VirtualBox application. The appliance has Spark 1.6.1 installed, and has all the necessary data files for this and subsequent exercises in the `~/data` directory.\n> \n> Alternatively, you can install Spark yourself. Download it from [spark.apache.org](http://spark.apache.org/downloads.html) -- make sure to select a prepackaged binary version, such as [Spark 1.6.1 for Hadoop 2.6](http://www.apache.org/dyn/closer.lua/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz). Extract the archive to some location on your system. Then, download the [data files](https://www.dropbox.com/s/un1zr1jg6buoe3a/data.zip?dl=0) for the labs and place them in `~/data`.\n> \n> **NOTE**: If you install Spark on Windows (not in a virtual machine), many things are going to be more difficult. Ask the instructor for advice if necessary.\n","dateUpdated":"Jun 22, 2016 7:50:45 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466612822217_-1511537037","id":"20160622-192702_2038901673","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In this lab, you will become acquainted with your Spark installation, and run your first Spark job &ndash; a multi-file word count.</p>\n<blockquote><p>The instructor should have explained how to install Spark on your machine. One option is to use the instructor's VirtualBox appliance, which you can import in the VirtualBox application. The appliance has Spark 1.6.1 installed, and has all the necessary data files for this and subsequent exercises in the <code>~/data</code> directory.</p>\n<p>Alternatively, you can install Spark yourself. Download it from <a href=\"http://spark.apache.org/downloads.html\">spark.apache.org</a> &ndash; make sure to select a prepackaged binary version, such as <a href=\"http://www.apache.org/dyn/closer.lua/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz\">Spark 1.6.1 for Hadoop 2.6</a>. Extract the archive to some location on your system. Then, download the <a href=\"https://www.dropbox.com/s/un1zr1jg6buoe3a/data.zip?dl=0\">data files</a> for the labs and place them in <code>~/data</code>.</p>\n<p><strong>NOTE</strong>: If you install Spark on Windows (not in a virtual machine), many things are going to be more difficult. Ask the instructor for advice if necessary.</p>\n</blockquote>\n"},"dateCreated":"Jun 22, 2016 7:27:02 PM","dateStarted":"Jun 22, 2016 7:50:13 PM","dateFinished":"Jun 22, 2016 7:50:13 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"text":"%md\n#### Task 1: Inspecting the Spark Installation\n\nOpen a terminal window. Navigate to the directory where you extracted Apache Spark. On the instructor-provided virtual machine, this is `~/spark`.\n\nInspect the files in the `bin` directory. You will soon use `pyspark` to launch your first Spark job. Also note `spark-submit`, which is used to submit standalone Spark programs to a cluster.\n\nInspect the scripts in the `sbin` directory. These scripts help with setting up a stand-alone Spark cluster, deploying Spark to EC2 virtual machines, and a bunch of additional tasks.\n\nFinally, take a look at the `examples` directory. You can find a number of stand-alone demo programs here, covering a variety of Spark APIs.\n","dateUpdated":"Jun 22, 2016 7:32:00 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466612195800_1445753567","id":"20160622-191635_396926758","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Task 1: Inspecting the Spark Installation</h4>\n<p>Open a terminal window. Navigate to the directory where you extracted Apache Spark. On the instructor-provided virtual machine, this is <code>~/spark</code>.</p>\n<p>Inspect the files in the <code>bin</code> directory. You will soon use <code>pyspark</code> to launch your first Spark job. Also note <code>spark-submit</code>, which is used to submit standalone Spark programs to a cluster.</p>\n<p>Inspect the scripts in the <code>sbin</code> directory. These scripts help with setting up a stand-alone Spark cluster, deploying Spark to EC2 virtual machines, and a bunch of additional tasks.</p>\n<p>Finally, take a look at the <code>examples</code> directory. You can find a number of stand-alone demo programs here, covering a variety of Spark APIs.</p>\n"},"dateCreated":"Jun 22, 2016 7:16:35 PM","dateStarted":"Jun 22, 2016 7:31:52 PM","dateFinished":"Jun 22, 2016 7:31:52 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"text":"%md\n\n#### Task 2: Inspecting the Lab Data Files\n\nIn this lab, you will implement a multi-file word count. The texts you will use are freely available books from [Project Gutenberg](http://www.gutenberg.org), including classics such as Lewis Carroll's \"Alice in Wonderland\" and Jane Austin's \"Pride and Prejudice\".\n\nTake a look at some of the text files in the `~/data` directory. From the terminal, run:\n\n```\nhead -n 50 ~/data/*.txt | less\n```\n\nThis shows the first 50 lines of each file. Press SPACE to scroll, or `q` to exit `less`.\n","dateUpdated":"Jun 22, 2016 7:32:00 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466612939210_-157657087","id":"20160622-192859_1394102286","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Task 2: Inspecting the Lab Data Files</h4>\n<p>In this lab, you will implement a multi-file word count. The texts you will use are freely available books from <a href=\"http://www.gutenberg.org\">Project Gutenberg</a>, including classics such as Lewis Carroll's &ldquo;Alice in Wonderland&rdquo; and Jane Austin's &ldquo;Pride and Prejudice&rdquo;.</p>\n<p>Take a look at some of the text files in the <code>~/data</code> directory. From the terminal, run:</p>\n<pre><code>head -n 50 ~/data/*.txt | less\n</code></pre>\n<p>This shows the first 50 lines of each file. Press SPACE to scroll, or <code>q</code> to exit <code>less</code>.</p>\n"},"dateCreated":"Jun 22, 2016 7:28:59 PM","dateStarted":"Jun 22, 2016 7:31:52 PM","dateFinished":"Jun 22, 2016 7:31:52 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"text":"%md\n\n#### Task 3: Implementing a Multi-File Word Count\n\nNavigate to the Spark installation directory, and run `bin/pyspark`. After a few seconds, you should see an interactive Python shell, which has a pre-initialized `SparkContext` object called `sc`.\n\n```\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n      /_/\n\nUsing Python version 2.7.6 (default, Jun 22 2015 17:58:13)\nSparkContext available as sc, HiveContext available as sqlContext.\n>>>\n```\n\nTo explore the available methods, run the following command:\n\n```python\ndir(sc)\n```\n\nIn this lab, you are going to use the `sc.textFile` method. To figure out what it does, run the following command:\n\n```python\nhelp(sc.textFile)\n```\n\nNote that even though it's not mentioned in the short documentation snippet you just read, the `textFile` method can also work with a directory path or a wildcard filter such as `/home/spark/Documents/spark-workshop/data/*.txt`.\n\n> Of course, if you are not using the instructor-supplied appliance, your `data` directory might reside in a different location.\n\nYour first task is to print out the number of lines in all the text files, combined. In general, you should try to come up with the solution yourself, and only then continue reading for the \"school\" solution.\n\n**Solution**:\n\n```python\nsc.textFile(\"/home/spark/Documents/spark-workshop/data/*.txt\").count()\n```\n","dateUpdated":"Jun 22, 2016 7:32:00 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466612954905_731143520","id":"20160622-192914_850045312","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Task 3: Implementing a Multi-File Word Count</h4>\n<p>Navigate to the Spark installation directory, and run <code>bin/pyspark</code>. After a few seconds, you should see an interactive Python shell, which has a pre-initialized <code>SparkContext</code> object called <code>sc</code>.</p>\n<pre><code>Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n      /_/\n\nUsing Python version 2.7.6 (default, Jun 22 2015 17:58:13)\nSparkContext available as sc, HiveContext available as sqlContext.\n&gt;&gt;&gt;\n</code></pre>\n<p>To explore the available methods, run the following command:</p>\n<pre><code class=\"python\">dir(sc)\n</code></pre>\n<p>In this lab, you are going to use the <code>sc.textFile</code> method. To figure out what it does, run the following command:</p>\n<pre><code class=\"python\">help(sc.textFile)\n</code></pre>\n<p>Note that even though it's not mentioned in the short documentation snippet you just read, the <code>textFile</code> method can also work with a directory path or a wildcard filter such as <code>/home/spark/Documents/spark-workshop/data/*.txt</code>.</p>\n<blockquote><p>Of course, if you are not using the instructor-supplied appliance, your <code>data</code> directory might reside in a different location.</p>\n</blockquote>\n<p>Your first task is to print out the number of lines in all the text files, combined. In general, you should try to come up with the solution yourself, and only then continue reading for the &ldquo;school&rdquo; solution.</p>\n<p><strong>Solution</strong>:</p>\n<pre><code class=\"python\">sc.textFile(\"/home/spark/Documents/spark-workshop/data/*.txt\").count()\n</code></pre>\n"},"dateCreated":"Jun 22, 2016 7:29:14 PM","dateStarted":"Jun 22, 2016 7:31:52 PM","dateFinished":"Jun 22, 2016 7:31:52 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"text":"%md\n\nGreat! Your next task is to implement the actual word-counting program. You've already seen one in class, and now it's time for your own. Print the top 10 most frequent words in the provided books.\n\n**Solution**:\n\n```python\nlines = sc.textFile(\"/home/vagrant/data/*.txt\")\nwords = lines.flatMap(lambda line: line.split())\npairs = words.map(lambda word: (word, 1))\nfreqs = pairs.reduceByKey(lambda a, b: a + b)\ntop10 = freqs.sortBy(lambda (word, count): -count).take(10)\nfor (word, count) in top10:\n    print(\"the word '%s' appears %d times\" % (word, count))\n```\n\nTo be honest, we don't really care about words like \"the\", \"a\", and \"of\". Ideally, we would have a list of stop words to ignore. For now, modify your solution to filter out words shorter than 4 characters.\n\nAdditionally, you might be wondering about the types of all these variables -- most of them are RDDs. To trace the lineage of an RDD, use the `toDebugString` method. For example, `print(freqs.toDebugString())` should display the logical plan for that RDD's evaluation. We will discuss some of these concepts later.\n","dateUpdated":"Jun 22, 2016 7:32:00 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613064486_431224913","id":"20160622-193104_135312799","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Great! Your next task is to implement the actual word-counting program. You've already seen one in class, and now it's time for your own. Print the top 10 most frequent words in the provided books.</p>\n<p><strong>Solution</strong>:</p>\n<pre><code class=\"python\">lines = sc.textFile(\"/home/vagrant/data/*.txt\")\nwords = lines.flatMap(lambda line: line.split())\npairs = words.map(lambda word: (word, 1))\nfreqs = pairs.reduceByKey(lambda a, b: a + b)\ntop10 = freqs.sortBy(lambda (word, count): -count).take(10)\nfor (word, count) in top10:\n    print(\"the word '%s' appears %d times\" % (word, count))\n</code></pre>\n<p>To be honest, we don't really care about words like &ldquo;the&rdquo;, &ldquo;a&rdquo;, and &ldquo;of&rdquo;. Ideally, we would have a list of stop words to ignore. For now, modify your solution to filter out words shorter than 4 characters.</p>\n<p>Additionally, you might be wondering about the types of all these variables &ndash; most of them are RDDs. To trace the lineage of an RDD, use the <code>toDebugString</code> method. For example, <code>print(freqs.toDebugString())</code> should display the logical plan for that RDD's evaluation. We will discuss some of these concepts later.</p>\n"},"dateCreated":"Jun 22, 2016 7:31:04 PM","dateStarted":"Jun 22, 2016 7:31:52 PM","dateFinished":"Jun 22, 2016 7:31:52 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"text":"%md\n\n#### Task 4: Run a Stand-Alone Spark Program\n\nYou're now ready to convert your multi-file word count into a stand-alone Spark program. Create a new file called `wordcount.py`.\n\nInitialize a `SparkContext` as follows:\n\n```python\nfrom pyspark import SparkContext\n\ndef run():\n    sc = SparkContext()\n    # TODO Your code goes here\n\nif __name__ == \"__main__\":\n    run()\n```\n\nNow, you can copy and paste your solution in the `run` method. Congratulations -- you have a stand-alone Spark program! To run it, navigate back to the Spark installation directory in your terminal, and run the following command:\n\n```\nbin/spark-submit --master 'local[*]' path/to/wordcount.py\n```\n\nYou should replace `path/to/wordcount.py` with the actual path on your system. If everything went fine, you should see a lot of diagnostic output, but somewhere buried in it would be your top 10 words.\n","dateUpdated":"Jun 22, 2016 7:32:00 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613085788_-504084339","id":"20160622-193125_31241328","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Task 4: Run a Stand-Alone Spark Program</h4>\n<p>You're now ready to convert your multi-file word count into a stand-alone Spark program. Create a new file called <code>wordcount.py</code>.</p>\n<p>Initialize a <code>SparkContext</code> as follows:</p>\n<pre><code class=\"python\">from pyspark import SparkContext\n\ndef run():\n    sc = SparkContext()\n    # TODO Your code goes here\n\nif __name__ == \"__main__\":\n    run()\n</code></pre>\n<p>Now, you can copy and paste your solution in the <code>run</code> method. Congratulations &ndash; you have a stand-alone Spark program! To run it, navigate back to the Spark installation directory in your terminal, and run the following command:</p>\n<pre><code>bin/spark-submit --master 'local[*]' path/to/wordcount.py\n</code></pre>\n<p>You should replace <code>path/to/wordcount.py</code> with the actual path on your system. If everything went fine, you should see a lot of diagnostic output, but somewhere buried in it would be your top 10 words.</p>\n"},"dateCreated":"Jun 22, 2016 7:31:25 PM","dateStarted":"Jun 22, 2016 7:31:52 PM","dateFinished":"Jun 22, 2016 7:31:52 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"text":"%md\n\n#### Discussion\n\nInstead of using `reduceByKey`, you could have used a method called `countByValue`. Read its documentation, and try to understand how it works. Would using it be a good idea?\n","dateUpdated":"Jun 22, 2016 7:32:00 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613099243_1420845056","id":"20160622-193139_932882047","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Discussion</h4>\n<p>Instead of using <code>reduceByKey</code>, you could have used a method called <code>countByValue</code>. Read its documentation, and try to understand how it works. Would using it be a good idea?</p>\n"},"dateCreated":"Jun 22, 2016 7:31:39 PM","dateStarted":"Jun 22, 2016 7:31:53 PM","dateFinished":"Jun 22, 2016 7:31:53 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"dateUpdated":"Jun 22, 2016 7:32:00 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466613104502_-1634846033","id":"20160622-193144_550730389","result":{"code":"SUCCESS","type":"TEXT"},"dateCreated":"Jun 22, 2016 7:31:44 PM","dateStarted":"Jun 22, 2016 7:31:53 PM","dateFinished":"Jun 22, 2016 7:31:53 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:68"}],"name":"Lab 1 - Multi-File Word Count","id":"2BRP5EQ5M","angularObjects":{"2BPTZXB14":[],"2BMJRDTW3":[],"2BKPUUQTH":[],"2BKSECM3A":[],"2BN9ZJDBP":[],"2BKNGQUUG":[],"2BKAYKVR2":[],"2BKSAS565":[],"2BMPZT5XH":[],"2BNWNU8NC":[],"2BQ1AJWE4":[],"2BN44SKDU":[],"2BN59C6S8":[],"2BP824Q3U":[]},"config":{"looknfeel":"default"},"info":{}}